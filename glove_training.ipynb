{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff7af9a2",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "No need to run, files are already generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffdfa1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/manishank/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3da2519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 6096 records into tab-separated format at datasets/english/english_final_test_cleaned.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from html import unescape\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def clean_english_text(text):\n",
    "    text = unescape(text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[0-9]', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "def clean_hindi_text(text):\n",
    "    text = unescape(text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[0-9]', ' ', text)\n",
    "    text = re.sub(r'[^\\u0900-\\u097F\\u1CD0-\\u1CFF\\uA8E0-\\uA8FF\\u0964\\u0965\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def parse_dataset(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    pattern = r\"'(.*?)','(.*?)'(?:\\n|$)\"\n",
    "    pairs = re.findall(pattern, content, re.DOTALL)\n",
    "    if not pairs:\n",
    "        lines = content.strip().split(\"\\n\")\n",
    "        pairs = [tuple(line.split(\",\", 1)) for line in lines if \",\" in line]\n",
    "    return pairs\n",
    "\n",
    "def clean_file_for_glove(input_path, output_path, lang='english'):\n",
    "    pairs = parse_dataset(input_path)\n",
    "    with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        for _, text in pairs:\n",
    "            cleaned = clean_english_text(text) if lang == 'english' else clean_hindi_text(text)\n",
    "            if cleaned:\n",
    "                outfile.write(cleaned + '\\n')\n",
    "\n",
    "def clean_file_with_labels(input_path, output_path, lang='english'):\n",
    "    pairs = parse_dataset(input_path)\n",
    "    with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        for label, text in pairs:\n",
    "            cleaned = clean_english_text(text) if lang == 'english' else clean_hindi_text(text)\n",
    "            if cleaned:\n",
    "                outfile.write(f\"{label}\\t{cleaned}\\n\")\n",
    "\n",
    "import re\n",
    "from html import unescape\n",
    "\n",
    "def convert_raw_to_tab(input_path, output_path):\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    pattern = r\"'([^']+)','(.*?)'(?:\\n|$)\"\n",
    "    pairs = re.findall(pattern, content, re.DOTALL)\n",
    "    with open(output_path, 'w', encoding='utf-8') as out:\n",
    "        for label, text in pairs:\n",
    "            text = clean_english_text(text)\n",
    "            label = label.strip()\n",
    "            text = unescape(text.strip().replace(\"\\n\", \" \"))\n",
    "            out.write(f\"{label}\\t{text}\\n\")\n",
    "    print(f\"Converted {len(pairs)} records into tab-separated format at {output_path}\")\n",
    "\n",
    "def clean_english_training():\n",
    "    datasets = [\n",
    "        ('datasets/english/english_2500.txt', 'datasets/english/cleaned_english_2500.txt'),\n",
    "        ('datasets/english/english_15000.txt', 'datasets/english/cleaned_english_15000.txt'),\n",
    "        ('datasets/english/english_30000.txt', 'datasets/english/cleaned_english_30000.txt'),\n",
    "    ]\n",
    "    for in_path, out_path in datasets:\n",
    "        clean_file_for_glove(in_path, out_path, 'english')\n",
    "\n",
    "def clean_hindi_training():\n",
    "    datasets = [\n",
    "        ('datasets/hindi/hindi_2500.txt', 'datasets/hindi/cleaned_hindi_2500.txt'),\n",
    "        ('datasets/hindi/hindi_15000.txt', 'datasets/hindi/cleaned_hindi_15000.txt'),\n",
    "        ('datasets/hindi/hindi_30000.txt', 'datasets/hindi/cleaned_hindi_30000.txt'),\n",
    "    ]\n",
    "    for in_path, out_path in datasets:\n",
    "        clean_file_for_glove(in_path, out_path, 'hindi')\n",
    "\n",
    "def clean_hindi_test():\n",
    "    datasets = [\n",
    "        ('datasets/hindi/hindi_test.txt', 'datasets/hindi/cleaned_hindi_test_labelled.txt'),\n",
    "    ]\n",
    "    for in_path, out_path in datasets:\n",
    "        clean_file_with_labels(in_path, out_path, 'hindi')\n",
    "\n",
    "def clean_english_training_labelled():\n",
    "    datasets = [\n",
    "        ('datasets/english/english_2500.txt', 'datasets/english/cleaned_english_2500_labelled.txt'),\n",
    "        ('datasets/english/english_15000.txt', 'datasets/english/cleaned_english_15000_labelled.txt'),\n",
    "        ('datasets/english/english_30000.txt', 'datasets/english/cleaned_english_30000_labelled.txt'),\n",
    "    ]\n",
    "    for in_path, out_path in datasets:\n",
    "        clean_file_with_labels(in_path, out_path, 'english')\n",
    "\n",
    "def clean_hindi_training_labelled():\n",
    "    datasets = [\n",
    "        ('datasets/hindi/hindi_2500.txt', 'datasets/hindi/cleaned_hindi_2500_labelled.txt'),\n",
    "        ('datasets/hindi/hindi_15000.txt', 'datasets/hindi/cleaned_hindi_15000_labelled.txt'),\n",
    "        ('datasets/hindi/hindi_30000.txt', 'datasets/hindi/cleaned_hindi_30000_labelled.txt'),\n",
    "    ]\n",
    "    for in_path, out_path in datasets:\n",
    "        clean_file_with_labels(in_path, out_path, 'hindi')\n",
    "\n",
    "clean_english_training()\n",
    "clean_hindi_training()\n",
    "clean_hindi_test()\n",
    "clean_english_training_labelled()\n",
    "clean_hindi_training_labelled()\n",
    "\n",
    "convert_raw_to_tab(\n",
    "    \"datasets/english/english_test.txt\", \n",
    "    \"datasets/english/english_final_test_cleaned.txt\"\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29943884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 28775 records into tab-separated format at datasets/english/english_final30k_train_cleaned.txt\n"
     ]
    }
   ],
   "source": [
    "convert_raw_to_tab(\n",
    "    \"datasets/english/english_30000.txt\", \n",
    "    \"datasets/english/english_final30k_train_cleaned.txt\"\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "458e70a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 15000 records into tab-separated format at datasets/english/english_final15k_train_cleaned.txt\n"
     ]
    }
   ],
   "source": [
    "convert_raw_to_tab(\n",
    "    \"datasets/english/english_15000.txt\", \n",
    "    \"datasets/english/english_final15k_train_cleaned.txt\"\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cd2e6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 2500 records into tab-separated format at datasets/english/english_final2.5k_train_cleaned.txt\n"
     ]
    }
   ],
   "source": [
    "convert_raw_to_tab(\n",
    "    \"datasets/english/english_2500.txt\", \n",
    "    \"datasets/english/english_final2.5k_train_cleaned.txt\"\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c3f135",
   "metadata": {},
   "source": [
    "# Building Co-Occurance matrix and saving it \n",
    "Necessary if we want to do the training of text to word embeddings model.\n",
    "Files of this are not given, are not necessary to use trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298dc7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import dok_matrix\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def load_our_data(filepath):\n",
    "    with open(filepath,'r',encoding = 'utf-8') as f:\n",
    "        return [line.strip().split() for line in f]\n",
    "\n",
    "def build_and_save_cooccurrence_matrix(training_data, output_dir, window=20):\n",
    "    print(f\"Loading data from {training_data}...\")\n",
    "    training = load_our_data(training_data)\n",
    "    print(\"Building vocabulary...\")\n",
    "    total_words = [word for sentence in training for word in sentence]\n",
    "    vocabulary = list(set(total_words))\n",
    "    word_to_id = {word: i for i,word in enumerate(vocabulary)}\n",
    "    id_to_word = {i:word for word,i in word_to_id.items()}\n",
    "    size = len(vocabulary)\n",
    "    print(f\"Vocabulary size: {size}\")\n",
    "    print(f\"Total sentences: {len(training)}\")\n",
    "    print(f\"Window size: {window}\")\n",
    "    print(\"Building co-occurrence matrix...\")\n",
    "    x = dok_matrix((size,size), dtype=np.float32)\n",
    "    for sentence_idx, sentence in enumerate(training):\n",
    "        if sentence_idx % 5000 == 0:\n",
    "            print(f\"Processed {sentence_idx}/{len(training)} sentences ({sentence_idx/len(training)*100:.1f}%)\")\n",
    "        indice = [word_to_id.get(w) for w in sentence if w in word_to_id]\n",
    "        for word_index, word in enumerate(indice):\n",
    "            start = max(0, word_index - window)\n",
    "            end = min(len(indice), word_index + window + 1)\n",
    "            for context_index in range(start, end):\n",
    "                if word_index == context_index:\n",
    "                    continue\n",
    "                distance = abs(context_index - word_index)\n",
    "                context_word = indice[context_index]\n",
    "                x[word, context_word] = x.get((word, context_word), 0) + 1.0 / distance\n",
    "    print(f\"Co-occurrence matrix built with {len(x.keys())} non-zero entries\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(\"Saving co-occurrence matrix...\")\n",
    "    non_zero_indices = list(x.keys())\n",
    "    co_occurrence_data = {\n",
    "        'indices': non_zero_indices,\n",
    "        'values': [x[idx] for idx in non_zero_indices]\n",
    "    }\n",
    "    with open(f\"{output_dir}/cooccurrence_data.pkl\", \"wb\") as f:\n",
    "        pickle.dump(co_occurrence_data, f)\n",
    "    print(\"Saving vocabulary...\")\n",
    "    with open(f\"{output_dir}/word_to_id.pkl\", \"wb\") as f:\n",
    "        pickle.dump(word_to_id, f)\n",
    "    with open(f\"{output_dir}/id_to_word.pkl\", \"wb\") as f:\n",
    "        pickle.dump(id_to_word, f)\n",
    "    metadata = {\n",
    "        'vocab_size': size,\n",
    "        'window_size': window,\n",
    "        'dataset': training_data,\n",
    "        'total_sentences': len(training),\n",
    "        'non_zero_entries': len(non_zero_indices)\n",
    "    }\n",
    "    with open(f\"{output_dir}/preprocessing_metadata.pkl\", \"wb\") as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    print(f\"All preprocessing data saved to {output_dir}/\")\n",
    "    print(f\"Files created:\")\n",
    "    print(f\"  - cooccurrence_data.pkl ({len(non_zero_indices)} entries)\")\n",
    "    print(f\"  - word_to_id.pkl ({size} words)\")\n",
    "    print(f\"  - id_to_word.pkl ({size} words)\")\n",
    "    print(f\"  - preprocessing_metadata.pkl\")\n",
    "    return output_dir\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    training_data = 'datasets/english/cleaned_english_30000.txt'\n",
    "    output_dir = \"models/matrixes/preprocessed_30k_w20\"\n",
    "    window = 20\n",
    "    build_and_save_cooccurrence_matrix(training_data, output_dir, window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153de516",
   "metadata": {},
   "source": [
    "# Training text to word embedding model using the Co-Occurance matrix\n",
    "Trained model files are in the same directory, No need to run this\n",
    "\n",
    "\n",
    "\n",
    " Note this code only works on cuda enabled systems\n",
    "\n",
    "\n",
    " Requirement: cupy-cuda12x or cupy-cuda13x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c76de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def load_preprocessed_data(data_dir):\n",
    "    print(f\"Loading preprocessed data from {data_dir}...\")\n",
    "    with open(f\"{data_dir}/cooccurrence_data.pkl\", \"rb\") as f:\n",
    "        co_occurrence_data = pickle.load(f)\n",
    "    with open(f\"{data_dir}/word_to_id.pkl\", \"rb\") as f:\n",
    "        word_to_id = pickle.load(f)\n",
    "    with open(f\"{data_dir}/id_to_word.pkl\", \"rb\") as f:\n",
    "        id_to_word = pickle.load(f)\n",
    "    with open(f\"{data_dir}/preprocessing_metadata.pkl\", \"rb\") as f:\n",
    "        metadata = pickle.load(f)\n",
    "    print(f\"Loaded:\")\n",
    "    print(f\"  - Vocabulary: {metadata['vocab_size']} words\")\n",
    "    print(f\"  - Co-occurrences: {metadata['non_zero_entries']} entries\")\n",
    "    print(f\"  - Window size: {metadata['window_size']}\")\n",
    "    print(f\"  - Dataset: {metadata['dataset']}\")\n",
    "    return co_occurrence_data, word_to_id, id_to_word, metadata\n",
    "\n",
    "def safe_gpu_init():\n",
    "    try:\n",
    "        print(\"Initializing GPU...\")\n",
    "        device_count = cp.cuda.runtime.getDeviceCount()\n",
    "        print(f\"Found {device_count} GPU(s)\")\n",
    "        cp.cuda.Device(0).use()\n",
    "        test = cp.array([1, 2, 3])\n",
    "        result = cp.sum(test)\n",
    "        print(f\"GPU available: {cp.cuda.Device()}\")\n",
    "        print(f\"Test operation successful: {result}\")\n",
    "        del test\n",
    "        try:\n",
    "            cp.get_default_memory_pool().free_all_blocks()\n",
    "            print(\"GPU memory cleared\")\n",
    "        except Exception as mem_error:\n",
    "            print(f\"Could not clear memory pool: {mem_error}\")\n",
    "            print(\"Continuing anyway...\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"GPU initialization failed: {e}\")\n",
    "        print(\"1. Check GPU status: nvidia-smi\")\n",
    "        print(\"2. Restart system: sudo reboot\")\n",
    "        print(\"3. Kill Python processes: sudo pkill -f python\")\n",
    "        print(\"4. Reset GPU: sudo nvidia-smi --gpu-reset\")\n",
    "        return False\n",
    "\n",
    "def train_glove_gpu(data_dir, dimension=100, epochs=100, learning_rate=0.01, \n",
    "                   x_max=100, alpha=0.75, batch_size=30000):\n",
    "    co_occurrence_data, word_to_id, id_to_word, metadata = load_preprocessed_data(data_dir)\n",
    "    if not safe_gpu_init():\n",
    "        print(\"Cannot proceed without GPU\")\n",
    "        return None, None, None, None\n",
    "    non_zero_indices = co_occurrence_data['indices']\n",
    "    non_zero_values = co_occurrence_data['values']\n",
    "    vocab_size = metadata['vocab_size']\n",
    "    print(f\"Training Configuration:\")\n",
    "    print(f\"  - Embedding dimension: {dimension}\")\n",
    "    print(f\"  - Epochs: {epochs}\")\n",
    "    print(f\"  - Learning rate: {learning_rate}\")\n",
    "    print(f\"  - Batch size: {batch_size}\")\n",
    "    def weighting_function(x):\n",
    "        return cp.where(x < x_max, (x/x_max) ** alpha, 1.0)\n",
    "    print(f\"Initializing embeddings ({vocab_size} x {dimension})...\")\n",
    "    try:\n",
    "        cp.random.seed(0)\n",
    "        word_vec = cp.random.uniform(-0.01, 0.01, (vocab_size, dimension), dtype=cp.float32)\n",
    "        context_vec = cp.random.uniform(-0.01, 0.01, (vocab_size, dimension), dtype=cp.float32)\n",
    "        b = cp.zeros(vocab_size, dtype=cp.float32)\n",
    "        c = cp.zeros(vocab_size, dtype=cp.float32)\n",
    "        print(\"Embeddings initialized on GPU\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to allocate embeddings: {e}\")\n",
    "        print(\"Try reducing dimension size\")\n",
    "        return None, None, None, None\n",
    "    print(\"Converting co-occurrence data to GPU...\")\n",
    "    try:\n",
    "        all_indices_i = cp.array([idx[0] for idx in non_zero_indices], dtype=cp.int32)\n",
    "        all_indices_j = cp.array([idx[1] for idx in non_zero_indices], dtype=cp.int32)\n",
    "        all_values = cp.array(non_zero_values, dtype=cp.float32)\n",
    "        print(f\"{len(non_zero_indices)} co-occurrences loaded to GPU\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load co-occurrence data to GPU: {e}\")\n",
    "        return None, None, None, None\n",
    "    def print_gpu_memory():\n",
    "        mempool = cp.get_default_memory_pool()\n",
    "        print(f\"GPU memory used: {mempool.used_bytes() / 1024**3:.2f} GB\")\n",
    "    print_gpu_memory()\n",
    "    print(f\"Starting training...\")\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_start in range(0, len(non_zero_indices), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, len(non_zero_indices))\n",
    "            indices_i = all_indices_i[batch_start:batch_end]\n",
    "            indices_j = all_indices_j[batch_start:batch_end]\n",
    "            values = all_values[batch_start:batch_end]\n",
    "            Xij = values\n",
    "            wgt = weighting_function(Xij)\n",
    "            word_vecs_batch = word_vec[indices_i]\n",
    "            context_vecs_batch = context_vec[indices_j]\n",
    "            b_batch = b[indices_i]\n",
    "            c_batch = c[indices_j]\n",
    "            dots = cp.sum(word_vecs_batch * context_vecs_batch, axis=1)\n",
    "            diff = dots + b_batch + c_batch - cp.log(Xij + 1)\n",
    "            batch_loss = cp.sum(wgt * diff ** 2)\n",
    "            total_loss += float(batch_loss)\n",
    "            grad = 2 * wgt * diff\n",
    "            grad_reshaped = grad.reshape(-1, 1)\n",
    "            word_vec_updates = grad_reshaped * context_vecs_batch\n",
    "            context_vec_updates = grad_reshaped * word_vecs_batch\n",
    "            cp.add.at(word_vec, indices_i, -learning_rate * word_vec_updates)\n",
    "            cp.add.at(context_vec, indices_j, -learning_rate * context_vec_updates)\n",
    "            cp.add.at(b, indices_i, -learning_rate * grad)\n",
    "            cp.add.at(c, indices_j, -learning_rate * grad)\n",
    "            del word_vecs_batch, context_vecs_batch, b_batch, c_batch\n",
    "            del dots, diff, grad, grad_reshaped, word_vec_updates, context_vec_updates\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch:3d}, Loss: {total_loss:.4f}\")\n",
    "            print_gpu_memory()\n",
    "    embeddings = word_vec + context_vec\n",
    "    embeddings_cpu = cp.asnumpy(embeddings)\n",
    "    print(f\"Training completed!\")\n",
    "    print(f\"Final embeddings shape: {embeddings_cpu.shape}\")\n",
    "    return embeddings_cpu, word_to_id, id_to_word, metadata\n",
    "\n",
    "def save_trained_model(embeddings, word_to_id, id_to_word, metadata, \n",
    "                      training_config, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    np.save(f\"{output_dir}/embeddings.npy\", embeddings)\n",
    "    with open(f\"{output_dir}/word_to_id.pkl\", \"wb\") as f:\n",
    "        pickle.dump(word_to_id, f)\n",
    "    with open(f\"{output_dir}/id_to_word.pkl\", \"wb\") as f:\n",
    "        pickle.dump(id_to_word, f)\n",
    "    complete_metadata = {**metadata, **training_config}\n",
    "    with open(f\"{output_dir}/metadata.pkl\", \"wb\") as f:\n",
    "        pickle.dump(complete_metadata, f)\n",
    "    print(f\"Model saved to {output_dir}/\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocessed_dir = \"models/matrixes/preprocessed_30k_w20\"\n",
    "    model_output_dir = \"models/test_to_word_embeddings/glove_model_30k\"\n",
    "    dimension = 300\n",
    "    epochs = 500\n",
    "    learning_rate = 0.005\n",
    "    batch_size = 10000\n",
    "    print(\"Starting GloVe Training\")\n",
    "    print(\"=\" * 40)\n",
    "    result = train_glove_gpu(\n",
    "        preprocessed_dir, \n",
    "        dimension=dimension,\n",
    "        epochs=epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    embeddings, word_to_id, id_to_word, metadata = result\n",
    "    if embeddings is not None:\n",
    "        print(\"Training successful! Saving model...\")\n",
    "        training_config = {\n",
    "            'embedding_dim': dimension,\n",
    "            'epochs': epochs,\n",
    "            'learning_rate': learning_rate,\n",
    "            'batch_size': batch_size\n",
    "        }\n",
    "        save_trained_model(embeddings, word_to_id, id_to_word, metadata, \n",
    "                          training_config, model_output_dir)\n",
    "        print(f\"Complete! Model saved to: {model_output_dir}\")\n",
    "    else:\n",
    "        print(\"Training failed!\")\n",
    "        print(\"Next steps:\")\n",
    "        print(\"1. Fix GPU issues (restart system)\")\n",
    "        print(\"2. Or reduce dimension/batch_size\")\n",
    "        print(\"3. Or try CPU-based training instead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56d9997",
   "metadata": {},
   "source": [
    "# Training the Random Forest classifier and saving pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a4ce224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x106a7da60>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /packages/35/31/47712f425c09cc8b8dba39c6c45aee939c4636a6feb8c81376a4eae653e0/tensorflow-2.20.0-cp312-cp312-macosx_12_0_arm64.whl.metadata\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x106a9d640>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /packages/35/31/47712f425c09cc8b8dba39c6c45aee939c4636a6feb8c81376a4eae653e0/tensorflow-2.20.0-cp312-cp312-macosx_12_0_arm64.whl.metadata\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x106a9d970>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known')': /packages/35/31/47712f425c09cc8b8dba39c6c45aee939c4636a6feb8c81376a4eae653e0/tensorflow-2.20.0-cp312-cp312-macosx_12_0_arm64.whl.metadata\u001b[0m\u001b[33m\n",
      "\u001b[0m  Downloading tensorflow-2.20.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (4.5 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in /Users/manishank/miniconda3/envs/nlp-env/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/manishank/miniconda3/envs/nlp-env/lib/python3.12/site-packages (from tensorflow) (25.0)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow)\n",
      "  Downloading protobuf-6.32.0-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/manishank/miniconda3/envs/nlp-env/lib/python3.12/site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in /Users/manishank/miniconda3/envs/nlp-env/lib/python3.12/site-packages (from tensorflow) (78.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/manishank/miniconda3/envs/nlp-env/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /Users/manishank/miniconda3/envs/nlp-env/lib/python3.12/site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/manishank/miniconda3/envs/nlp-env/lib/python3.12/site-packages (from tensorflow) (1.17.3)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.74.0-cp312-cp312-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Downloading keras-3.11.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/manishank/miniconda3/envs/nlp-env/lib/python3.12/site-packages (from tensorflow) (2.3.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/manishank/miniconda3/envs/nlp-env/lib/python3.12/site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /Users/manishank/miniconda3/envs/nlp-env/lib/python3.12/site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/manishank/miniconda3/envs/nlp-env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/manishank/miniconda3/envs/nlp-env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/manishank/miniconda3/envs/nlp-env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/manishank/miniconda3/envs/nlp-env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading markdown-3.9-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pillow in /Users/manishank/miniconda3/envs/nlp-env/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/manishank/miniconda3/envs/nlp-env/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /Users/manishank/miniconda3/envs/nlp-env/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow) (14.1.0)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Downloading optree-0.17.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/manishank/miniconda3/envs/nlp-env/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/manishank/miniconda3/envs/nlp-env/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/manishank/miniconda3/envs/nlp-env/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/manishank/miniconda3/envs/nlp-env/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Downloading tensorflow-2.20.0-cp312-cp312-macosx_12_0_arm64.whl (200.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m  \u001b[33m0:01:36\u001b[0mm0:00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.74.0-cp312-cp312-macosx_11_0_universal2.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading keras-3.11.3-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown-3.9-py3-none-any.whl (107 kB)\n",
      "Downloading protobuf-6.32.0-cp39-abi3-macosx_10_9_universal2.whl (426 kB)\n",
      "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.17.0-cp312-cp312-macosx_11_0_arm64.whl (351 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, werkzeug, termcolor, tensorboard-data-server, protobuf, optree, markdown, grpcio, google_pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/17\u001b[0m [tensorflow]7\u001b[0m [tensorflow]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google_pasta-0.2.0 grpcio-1.74.0 keras-3.11.3 libclang-18.1.1 markdown-3.9 namex-0.1.0 optree-0.17.0 protobuf-6.32.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.1.0 werkzeug-3.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a0d746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNN with models/text_to_word_embeddings/glove_model_hindi_30k_d150...\n",
      "Configuration:\n",
      "  - Max sequence length: 600\n",
      "  - Filter sizes: [2, 3, 4, 5, 6]\n",
      "  - Number of filters per size: 128\n",
      "  - Batch size: 32\n",
      "  - Learning rate: 0.001\n",
      "  - Weight decay (L2): 0.001\n",
      "  - Dropout: 0.3\n",
      "  - Epochs: 5\n",
      "Loading dataset from datasets/hindi/cleaned_hindi_test_labelled.txt...\n",
      "Loaded 30000 samples\n",
      "Label distribution before filtering:\n",
      "  [entertainment]: 3431 samples\n",
      "  [politics: 3408 samples\n",
      "  [national]: 3068 samples\n",
      "  [miscellaneous]: 2744 samples\n",
      "  [world: 2579 samples\n",
      "  [business]: 2228 samples\n",
      "  [technology]: 2201 samples\n",
      "  [world]: 2100 samples\n",
      "  [sports: 1657 samples\n",
      "  [विश्व_कप_2023: 1572 samples\n",
      "  [business: 1388 samples\n",
      "  [national: 1003 samples\n",
      "  [miscellaneous: 975 samples\n",
      "  [politics]: 628 samples\n",
      "  [sports]: 561 samples\n",
      "  [entertainment: 208 samples\n",
      "  [hatke: 186 samples\n",
      "  [automobile: 37 samples\n",
      "  [technology: 13 samples\n",
      "  [एशियन_गेम्स_2022: 11 samples\n",
      "  [एशिया_कप_2023: 2 samples\n",
      "Filtered out 2 samples with rare labels\n",
      "Final dataset size: 29998 samples\n",
      "Dataset info:\n",
      "  - Sequences shape: torch.Size([29998, 67])\n",
      "  - Number of classes: 20\n",
      "  - Vocabulary size: 16844\n",
      "  - Embedding dimension: 150\n",
      "Final label distribution:\n",
      "  [entertainment]: 3431 samples\n",
      "  [politics: 3408 samples\n",
      "  [national]: 3068 samples\n",
      "  [miscellaneous]: 2744 samples\n",
      "  [world: 2579 samples\n",
      "  [business]: 2228 samples\n",
      "  [technology]: 2201 samples\n",
      "  [world]: 2100 samples\n",
      "  [sports: 1657 samples\n",
      "  [विश्व_कप_2023: 1572 samples\n",
      "  [business: 1388 samples\n",
      "  [national: 1003 samples\n",
      "  [miscellaneous: 975 samples\n",
      "  [politics]: 628 samples\n",
      "  [sports]: 561 samples\n",
      "  [entertainment: 208 samples\n",
      "  [hatke: 186 samples\n",
      "  [automobile: 37 samples\n",
      "  [technology: 13 samples\n",
      "  [एशियन_गेम्स_2022: 11 samples\n",
      "Train-Test Split:\n",
      "  - Training samples: 23998\n",
      "  - Test samples: 6000\n",
      "  - Train ratio: 80.0%\n",
      "  - Test ratio: 20.0%\n",
      "Training set class distribution:\n",
      "  [automobile: 30 samples (weight: 2.553)\n",
      "  [business: 1110 samples (weight: 0.069)\n",
      "  [business]: 1782 samples (weight: 0.043)\n",
      "  [entertainment: 166 samples (weight: 0.461)\n",
      "  [entertainment]: 2745 samples (weight: 0.028)\n",
      "  [hatke: 149 samples (weight: 0.514)\n",
      "  [miscellaneous: 780 samples (weight: 0.098)\n",
      "  [miscellaneous]: 2195 samples (weight: 0.035)\n",
      "  [national: 802 samples (weight: 0.096)\n",
      "  [national]: 2454 samples (weight: 0.031)\n",
      "  [politics: 2726 samples (weight: 0.028)\n",
      "  [politics]: 502 samples (weight: 0.153)\n",
      "  [sports: 1326 samples (weight: 0.058)\n",
      "  [sports]: 449 samples (weight: 0.171)\n",
      "  [technology: 11 samples (weight: 6.964)\n",
      "  [technology]: 1761 samples (weight: 0.043)\n",
      "  [world: 2063 samples (weight: 0.037)\n",
      "  [world]: 1680 samples (weight: 0.046)\n",
      "  [एशियन_गेम्स_2022: 9 samples (weight: 8.511)\n",
      "  [विश्व_कप_2023: 1258 samples (weight: 0.061)\n",
      "Test set class distribution:\n",
      "  [automobile: 7 samples\n",
      "  [business: 278 samples\n",
      "  [business]: 446 samples\n",
      "  [entertainment: 42 samples\n",
      "  [entertainment]: 686 samples\n",
      "  [hatke: 37 samples\n",
      "  [miscellaneous: 195 samples\n",
      "  [miscellaneous]: 549 samples\n",
      "  [national: 201 samples\n",
      "  [national]: 614 samples\n",
      "  [politics: 682 samples\n",
      "  [politics]: 126 samples\n",
      "  [sports: 331 samples\n",
      "  [sports]: 112 samples\n",
      "  [technology: 2 samples\n",
      "  [technology]: 440 samples\n",
      "  [world: 516 samples\n",
      "  [world]: 420 samples\n",
      "  [एशियन_गेम्स_2022: 2 samples\n",
      "  [विश्व_कप_2023: 314 samples\n",
      "Using device: mps\n",
      "Epoch  5/5, Loss: 0.2123 (↓16.15%), Train Acc: 95.44%, Test Acc: 95.23%\n",
      "    New best test accuracy: 95.23% (saved)\n",
      "    Checkpoint saved: rnn/cnn_model_epoch_5.pkl\n",
      "FINAL EVALUATION ON TEST SET\n",
      "Test Accuracy:           0.9523 (95.23%)\n",
      "Weighted Precision:      0.9589\n",
      "Weighted Recall:         0.9523\n",
      "Weighted F1-Score:       0.9542\n",
      "Macro Precision:         0.9324\n",
      "Macro Recall:            0.9690\n",
      "Macro F1-Score:          0.9441\n",
      "Best Test Accuracy:      95.23%\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "      [automobile      1.000     1.000     1.000         7\n",
      "        [business      0.942     1.000     0.970       278\n",
      "       [business]      0.984     0.975     0.980       446\n",
      "   [entertainment      0.416     1.000     0.587        42\n",
      "  [entertainment]      0.995     0.914     0.953       686\n",
      "           [hatke      0.925     1.000     0.961        37\n",
      "   [miscellaneous      0.975     0.985     0.980       195\n",
      "  [miscellaneous]      0.963     0.951     0.957       549\n",
      "        [national      0.952     0.980     0.966       201\n",
      "       [national]      0.936     0.927     0.931       614\n",
      "        [politics      0.968     0.918     0.942       682\n",
      "       [politics]      0.831     0.976     0.898       126\n",
      "          [sports      0.976     0.988     0.982       331\n",
      "         [sports]      0.920     0.920     0.920       112\n",
      "      [technology      1.000     1.000     1.000         2\n",
      "     [technology]      1.000     0.970     0.985       440\n",
      "           [world      0.964     0.948     0.956       516\n",
      "          [world]      0.915     0.976     0.945       420\n",
      "[एशियन_गेम्स_2022      1.000     1.000     1.000         2\n",
      "   [विश्व_कप_2023      0.987     0.952     0.969       314\n",
      "\n",
      "         accuracy                          0.952      6000\n",
      "        macro avg      0.932     0.969     0.944      6000\n",
      "     weighted avg      0.959     0.952     0.954      6000\n",
      "\n",
      "[automobile              : 1.000 (100.0%)\n",
      "[business                : 1.000 (100.0%)\n",
      "[business]               : 0.975 (97.5%)\n",
      "[entertainment           : 1.000 (100.0%)\n",
      "[entertainment]          : 0.914 (91.4%)\n",
      "[hatke                   : 1.000 (100.0%)\n",
      "[miscellaneous           : 0.985 (98.5%)\n",
      "[miscellaneous]          : 0.951 (95.1%)\n",
      "[national                : 0.980 (98.0%)\n",
      "[national]               : 0.927 (92.7%)\n",
      "[politics                : 0.918 (91.8%)\n",
      "[politics]               : 0.976 (97.6%)\n",
      "[sports                  : 0.988 (98.8%)\n",
      "[sports]                 : 0.920 (92.0%)\n",
      "[technology              : 1.000 (100.0%)\n",
      "[technology]             : 0.970 (97.0%)\n",
      "[world                   : 0.948 (94.8%)\n",
      "[world]                  : 0.976 (97.6%)\n",
      "[एशियन_गेम्स_2022        : 1.000 (100.0%)\n",
      "[विश्व_कप_2023           : 0.952 (95.2%)\n",
      "Total predictions: 6000\n",
      "Correct predictions: 5714\n",
      "Incorrect predictions: 286\n",
      "Total epochs completed: 5\n",
      "Early stopped: No\n",
      "Final training accuracy: 95.44%\n",
      "Final training loss: 0.2123\n",
      "Best training loss: 0.2123\n",
      "Training improvement: 37.17%\n",
      "Checkpoints saved in: rnn/\n",
      "Final model saved to models/classification_models/hindi30k_cnn.pkl\n",
      "Checkpoints available in rnn/ folder\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def load_glove_model(model_dir):\n",
    "    embeddings = np.load(f\"{model_dir}/embeddings.npy\")\n",
    "    with open(f\"{model_dir}/word_to_id.pkl\", \"rb\") as f:\n",
    "        word_to_id = pickle.load(f)\n",
    "    with open(f\"{model_dir}/id_to_word.pkl\", \"rb\") as f:\n",
    "        id_to_word = pickle.load(f)\n",
    "    with open(f\"{model_dir}/metadata.pkl\", \"rb\") as f:\n",
    "        metadata = pickle.load(f)\n",
    "    return embeddings, word_to_id, id_to_word, metadata\n",
    "\n",
    "def text_to_indices(text, word_to_id, max_seq_len=600):\n",
    "    tokens = text.lower().split()\n",
    "    indices = [word_to_id.get(w, 0) for w in tokens]\n",
    "    if len(indices) > max_seq_len:\n",
    "        indices = indices[:max_seq_len]\n",
    "    return indices\n",
    "\n",
    "def prepare_cnn_dataset(file_path, word_to_id, max_seq_len=600):\n",
    "    sequences, labels = [], []\n",
    "    print(f\"Loading dataset from {file_path}...\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split('\\t', 1)\n",
    "            if len(parts) < 2:\n",
    "                print(f\"Warning: Skipping malformed line {line_num}\")\n",
    "                continue\n",
    "            genres_part = parts[0]\n",
    "            text_part = parts[1]\n",
    "            primary_genre = genres_part.split(',')[0].strip()\n",
    "            indices = text_to_indices(text_part, word_to_id, max_seq_len)\n",
    "            sequences.append(torch.tensor(indices, dtype=torch.long))\n",
    "            labels.append(primary_genre)\n",
    "    print(f\"Loaded {len(sequences)} samples\")\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    labels_array = np.array(labels)\n",
    "    counts = Counter(labels_array)\n",
    "    print(f\"Label distribution before filtering:\")\n",
    "    for label, count in sorted(counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {label}: {count} samples\")\n",
    "    filtered_indices = [i for i, label in enumerate(labels_array) if counts[label] >= 3]\n",
    "    print(f\"Filtered out {len(labels_array) - len(filtered_indices)} samples with rare labels\")\n",
    "    print(f\"Final dataset size: {len(filtered_indices)} samples\")\n",
    "    return padded_sequences[filtered_indices], labels_array[filtered_indices]\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes, pretrained_embeddings=None, \n",
    "                 filter_sizes=[3, 4, 5], num_filters=128, dropout=0.5, max_seq_len=600):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(embedding_dim, num_filters, kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        conv_outputs = []\n",
    "        for conv in self.convs:\n",
    "            conv_out = F.relu(conv(x))\n",
    "            pooled = F.max_pool1d(conv_out, kernel_size=conv_out.size(2))\n",
    "            conv_outputs.append(pooled.squeeze(2))\n",
    "        x = torch.cat(conv_outputs, dim=1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def train_cnn_model(model_dir, dataset_path, output_pkl, max_seq_len=600, \n",
    "                   filter_sizes=[3,4,5], num_filters=128, epochs=30, \n",
    "                   batch_size=32, lr=0.001, dropout=0.3, weight_decay=0.0005):\n",
    "    print(f\"Training CNN with {model_dir}...\")\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  - Max sequence length: {max_seq_len}\")\n",
    "    print(f\"  - Filter sizes: {filter_sizes}\")\n",
    "    print(f\"  - Number of filters per size: {num_filters}\")\n",
    "    print(f\"  - Batch size: {batch_size}\")\n",
    "    print(f\"  - Learning rate: {lr}\")\n",
    "    print(f\"  - Weight decay (L2): {weight_decay}\")\n",
    "    print(f\"  - Dropout: {dropout}\")\n",
    "    print(f\"  - Epochs: {epochs}\")\n",
    "    embeddings, word_to_id, id_to_word, metadata = load_glove_model(model_dir)\n",
    "    vocab_size, embedding_dim = embeddings.shape\n",
    "    X, y = prepare_cnn_dataset(dataset_path, word_to_id, max_seq_len)\n",
    "    unique_labels = sorted(set(y))\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "    y_idx = np.array([label_to_idx[label] for label in y])\n",
    "    print(f\"Dataset info:\")\n",
    "    print(f\"  - Sequences shape: {X.shape}\")\n",
    "    print(f\"  - Number of classes: {len(unique_labels)}\")\n",
    "    print(f\"  - Vocabulary size: {vocab_size}\")\n",
    "    print(f\"  - Embedding dimension: {embedding_dim}\")\n",
    "    print(f\"Final label distribution:\")\n",
    "    final_counts = Counter(y)\n",
    "    for label, count in sorted(final_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {label}: {count} samples\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_idx, test_size=0.2, random_state=42, stratify=y_idx\n",
    "    )\n",
    "    print(f\"Train-Test Split:\")\n",
    "    print(f\"  - Training samples: {len(X_train)}\")\n",
    "    print(f\"  - Test samples: {len(X_test)}\")\n",
    "    print(f\"  - Train ratio: {len(X_train)/len(X)*100:.1f}%\")\n",
    "    print(f\"  - Test ratio: {len(X_test)/len(X)*100:.1f}%\")\n",
    "    class_counts = np.bincount(y_train)\n",
    "    class_weights = 1. / class_counts\n",
    "    class_weights = class_weights / class_weights.sum() * len(class_weights)\n",
    "    weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "    print(f\"Training set class distribution:\")\n",
    "    for i, (label, count) in enumerate(zip([idx_to_label[i] for i in range(len(unique_labels))], class_counts)):\n",
    "        print(f\"  {label}: {count} samples (weight: {class_weights[i]:.3f})\")\n",
    "    print(f\"Test set class distribution:\")\n",
    "    test_class_counts = np.bincount(y_test)\n",
    "    for i, (label, count) in enumerate(zip([idx_to_label[i] for i in range(len(unique_labels))], test_class_counts)):\n",
    "        print(f\"  {label}: {count} samples\")\n",
    "    train_dataset = TensorDataset(X_train, torch.tensor(y_train, dtype=torch.long))\n",
    "    test_dataset = TensorDataset(X_test, torch.tensor(y_test, dtype=torch.long))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "     device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "     device = torch.device(\"cuda\")\n",
    "    else:\n",
    "     device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = TextCNN(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_classes=len(unique_labels),\n",
    "        pretrained_embeddings=embeddings,\n",
    "        filter_sizes=filter_sizes,\n",
    "        num_filters=num_filters,\n",
    "        dropout=dropout,\n",
    "        max_seq_len=max_seq_len\n",
    "    ).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    checkpoint_dir = \"rnn\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    best_test_acc = 0.0\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    patience_loss = 3\n",
    "    patience_acc = 3\n",
    "    min_loss_improvement = 0.005\n",
    "    best_loss = float('inf')\n",
    "    epochs_without_loss_improvement = 0\n",
    "    epochs_without_acc_improvement = 0\n",
    "    early_stopped = False\n",
    "    best_model_state = None\n",
    "    best_model_epoch = 0\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_accuracy = 100. * correct / total\n",
    "        train_losses.append(avg_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        loss_improvement = 0.0\n",
    "        if epoch == 0:\n",
    "            best_loss = avg_loss\n",
    "            epochs_without_loss_improvement = 0\n",
    "        else:\n",
    "            if avg_loss < best_loss:\n",
    "                loss_improvement = (best_loss - avg_loss) / best_loss\n",
    "                if loss_improvement >= min_loss_improvement:\n",
    "                    best_loss = avg_loss\n",
    "                    epochs_without_loss_improvement = 0\n",
    "                else:\n",
    "                    epochs_without_loss_improvement += 1\n",
    "            else:\n",
    "                epochs_without_loss_improvement += 1\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            with torch.no_grad():\n",
    "                for data, target in test_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)\n",
    "                    pred = output.argmax(dim=1)\n",
    "                    val_correct += pred.eq(target).sum().item()\n",
    "                    val_total += target.size(0)\n",
    "            val_accuracy = 100. * val_correct / val_total\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            print(f'Epoch {epoch+1:2d}/{epochs}, Loss: {avg_loss:.4f} (↓{loss_improvement*100:.2f}%), Train Acc: {train_accuracy:.2f}%, Test Acc: {val_accuracy:.2f}%')\n",
    "            if val_accuracy > best_test_acc:\n",
    "                best_test_acc = val_accuracy\n",
    "                epochs_without_acc_improvement = 0\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                best_model_epoch = epoch + 1\n",
    "                print(f\"    New best test accuracy: {val_accuracy:.2f}% (saved)\")\n",
    "            else:\n",
    "                epochs_without_acc_improvement += 1\n",
    "                print(f\"    No test accuracy improvement for {epochs_without_acc_improvement} epoch(s)\")\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"cnn_model_epoch_{epoch+1}.pkl\")\n",
    "            checkpoint_data = {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "                \"train_loss\": avg_loss,\n",
    "                \"train_accuracy\": train_accuracy,\n",
    "                \"val_accuracy\": val_accuracy,\n",
    "                \"label_to_idx\": label_to_idx,\n",
    "                \"idx_to_label\": idx_to_label,\n",
    "                \"vocab_size\": vocab_size,\n",
    "                \"embedding_dim\": embedding_dim,\n",
    "                \"num_classes\": len(unique_labels),\n",
    "                \"filter_sizes\": filter_sizes,\n",
    "                \"num_filters\": num_filters,\n",
    "                \"max_seq_len\": max_seq_len,\n",
    "                \"dropout\": dropout,\n",
    "                \"weight_decay\": weight_decay\n",
    "            }\n",
    "            with open(checkpoint_path, \"wb\") as f:\n",
    "                pickle.dump(checkpoint_data, f)\n",
    "            print(f\"    Checkpoint saved: {checkpoint_path}\")\n",
    "            model.train()\n",
    "        if epoch >= 4:\n",
    "            if epochs_without_acc_improvement >= patience_acc:\n",
    "                print(f\"Early stopping triggered after epoch {epoch+1}\")\n",
    "                print(f\"Test accuracy hasn't improved for {epochs_without_acc_improvement} validation checks\")\n",
    "                print(f\"Best test accuracy: {best_test_acc:.2f}% at epoch {best_model_epoch}\")\n",
    "                early_stopped = True\n",
    "                break\n",
    "            if epochs_without_loss_improvement >= patience_loss:\n",
    "                print(f\"Early stopping triggered after epoch {epoch+1}\")\n",
    "                print(f\"Loss improvement of {loss_improvement*100:.3f}% is below threshold of {min_loss_improvement*100:.1f}%\")\n",
    "                print(f\"No significant loss improvement for {epochs_without_loss_improvement} epochs\")\n",
    "                early_stopped = True\n",
    "                break\n",
    "    if early_stopped and best_model_state is not None:\n",
    "        print(f\"Restoring best model from epoch {best_model_epoch}\")\n",
    "        model.load_state_dict(best_model_state)\n",
    "    print(f\"FINAL EVALUATION ON TEST SET\")\n",
    "    model.eval()\n",
    "    y_true, y_pred, y_probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            probs = F.softmax(output, dim=1)\n",
    "            pred = output.argmax(dim=1)\n",
    "            y_true.extend(target.cpu().numpy())\n",
    "            y_pred.extend(pred.cpu().numpy())\n",
    "            y_probs.extend(probs.cpu().numpy())\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "    print(f'Test Accuracy:           {acc:.4f} ({acc*100:.2f}%)')\n",
    "    print(f'Weighted Precision:      {precision:.4f}')\n",
    "    print(f'Weighted Recall:         {recall:.4f}')\n",
    "    print(f'Weighted F1-Score:       {f1:.4f}')\n",
    "    print(f'Macro Precision:         {macro_precision:.4f}')\n",
    "    print(f'Macro Recall:            {macro_recall:.4f}')\n",
    "    print(f'Macro F1-Score:          {macro_f1:.4f}')\n",
    "    print(f'Best Test Accuracy:      {best_test_acc:.2f}%')\n",
    "    target_names = [idx_to_label[i] for i in sorted(set(y_true + y_pred))]\n",
    "    print(classification_report(y_true, y_pred, target_names=target_names, digits=3))\n",
    "    for i in sorted(set(y_true + y_pred)):\n",
    "        class_mask = np.array(y_true) == i\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_acc = np.sum(np.array(y_pred)[class_mask] == i) / np.sum(class_mask)\n",
    "            print(f'{idx_to_label[i]:<25}: {class_acc:.3f} ({class_acc*100:.1f}%)')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(f'Total predictions: {len(y_true)}')\n",
    "    print(f'Correct predictions: {np.trace(cm)}')\n",
    "    print(f'Incorrect predictions: {len(y_true) - np.trace(cm)}')\n",
    "    print(f'Total epochs completed: {len(train_accuracies)}')\n",
    "    print(f'Early stopped: {\"Yes\" if early_stopped else \"No\"}')\n",
    "    if early_stopped:\n",
    "        print(f'Best model from epoch: {best_model_epoch}')\n",
    "        print(f'Best test accuracy: {best_test_acc:.2f}%')\n",
    "    print(f'Final training accuracy: {train_accuracies[-1]:.2f}%')\n",
    "    print(f'Final training loss: {train_losses[-1]:.4f}')\n",
    "    print(f'Best training loss: {min(train_losses):.4f}')\n",
    "    print(f'Training improvement: {train_accuracies[-1] - train_accuracies[0]:.2f}%')\n",
    "    print(f'Checkpoints saved in: {checkpoint_dir}/')\n",
    "    final_model_path = output_pkl\n",
    "    model_data = {\n",
    "        \"final_epoch\": len(train_accuracies),\n",
    "        \"best_epoch\": best_model_epoch if early_stopped else len(train_accuracies),\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"label_to_idx\": label_to_idx,\n",
    "        \"idx_to_label\": idx_to_label,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"embedding_dim\": embedding_dim,\n",
    "        \"num_classes\": len(unique_labels),\n",
    "        \"filter_sizes\": filter_sizes,\n",
    "        \"num_filters\": num_filters,\n",
    "        \"max_seq_len\": max_seq_len,\n",
    "        \"dropout\": dropout,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"final_train_loss\": train_losses[-1],\n",
    "        \"final_train_accuracy\": train_accuracies[-1],\n",
    "        \"best_val_accuracy\": best_test_acc,\n",
    "        \"early_stopped\": early_stopped,\n",
    "        \"test_accuracy\": acc,\n",
    "        \"test_f1\": f1,\n",
    "        \"val_accuracies\": val_accuracies,\n",
    "        \"train_losses_history\": train_losses,\n",
    "        \"train_accuracies_history\": train_accuracies\n",
    "    }\n",
    "    with open(final_model_path, \"wb\") as f:\n",
    "        pickle.dump(model_data, f)\n",
    "    print(f\"Final model saved to {final_model_path}\")\n",
    "    print(f\"Checkpoints available in {checkpoint_dir}/ folder\")\n",
    "    return model, acc, f1\n",
    "\n",
    "models_to_train = [\n",
    "    {\n",
    "        \"model_dir\": \"embeddings/glove_model_30k\",\n",
    "        \"dataset_path\": \"datasets/hindi/cleaned_hindi_test_labelled.txt\",\n",
    "        \"output_pkl\": \"models/classification_models/engnew30k_cnn.pkl\",\n",
    "        \"max_seq_len\": 600,\n",
    "        \"filter_sizes\": [2, 3, 4, 5, 6],\n",
    "        \"num_filters\": 128,\n",
    "        \"epochs\": 5,\n",
    "        \"batch_size\": 32,\n",
    "        \"lr\": 0.001,\n",
    "        \"weight_decay\": 0.001\n",
    "    }\n",
    "]\n",
    "for config in models_to_train:\n",
    "    train_cnn_model(**config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
