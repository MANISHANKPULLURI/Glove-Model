{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35a9e12b",
   "metadata": {},
   "source": [
    "# Cosine similarity testing for pre-trained text to word embedding model\n",
    "Checking  similarity scores after trianing Glove model enter 1 for english and 2 for hindi and then enter word,by default using 30k datasets for both hindi and telugu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb171fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select GloVe model:\n",
      "1. English (30k dataset)\n",
      "2. Hindi   (30k dataset)\n",
      "\n",
      "Model loaded: wordembeddings/glove_model_30k\n",
      "Vocabulary size: 120821\n",
      "Embedding dimension: 300\n",
      "\n",
      "Vocabulary sample (first 20 words):\n",
      "finis instill hyatt sneezed jari naki dragoons maharani embellishing noodleman batasi retracing enhancing stillo cobain kinema muska inept xing technodrome \n",
      "... and 120801 more words\n",
      "\n",
      "Testing predefined words:\n",
      "\n",
      "Testing similarity for: action\n",
      "\n",
      "Most similar words to 'action':\n",
      "1. devta: 0.7613\n",
      "2. galvanised: 0.6599\n",
      "3. hiraasat: 0.6404\n",
      "4. taqdeer: 0.6271\n",
      "5. rangbaaz: 0.6222\n",
      "\n",
      "Testing similarity for: love\n",
      "\n",
      "Most similar words to 'love':\n",
      "1. falls: 0.6686\n",
      "2. fall: 0.6582\n",
      "3. married: 0.6247\n",
      "4. marry: 0.6217\n",
      "5. loves: 0.6194\n",
      "\n",
      "Testing similarity for: comedy\n",
      "\n",
      "Most similar words to 'comedy':\n",
      "1. errors: 0.8015\n",
      "2. slapstick: 0.7223\n",
      "3. screwball: 0.6545\n",
      "4. frothy: 0.6169\n",
      "5. drama: 0.5875\n",
      "\n",
      "Testing similarity for: school\n",
      "\n",
      "Most similar words to 'school':\n",
      "1. sohoku: 0.7582\n",
      "2. high: 0.6997\n",
      "3. teacher: 0.6747\n",
      "4. students: 0.6621\n",
      "5. student: 0.6520\n",
      "\n",
      "Testing similarity for: romance\n",
      "\n",
      "Most similar words to 'romance':\n",
      "1. blossoming: 0.7433\n",
      "2. rekindle: 0.7330\n",
      "3. shipboard: 0.7200\n",
      "4. budding: 0.6721\n",
      "5. blossoms: 0.6527\n",
      "\n",
      "Testing similarity for: movie\n",
      "\n",
      "Most similar words to 'movie':\n",
      "1. film: 0.7953\n",
      "2. kadaksham: 0.6540\n",
      "3. chammak: 0.6390\n",
      "4. ends: 0.6390\n",
      "5. disruptly: 0.6279\n",
      "\n",
      "Testing similarity for: film\n",
      "\n",
      "Most similar words to 'film':\n",
      "1. movie: 0.7953\n",
      "2. donsky: 0.7455\n",
      "3. usire: 0.6735\n",
      "4. story: 0.6702\n",
      "5. ends: 0.6488\n",
      "\n",
      "Testing similarity for: good\n",
      "\n",
      "Most similar words to 'good':\n",
      "1. riddance: 0.7104\n",
      "2. pattukottai: 0.7010\n",
      "3. naturedly: 0.6997\n",
      "4. bad: 0.6810\n",
      "5. samaritan: 0.6729\n",
      "\n",
      "Testing similarity for: bad\n",
      "\n",
      "Most similar words to 'bad':\n",
      "1. good: 0.6810\n",
      "2. luck: 0.6651\n",
      "3. guys: 0.6398\n",
      "4. yardie: 0.6232\n",
      "5. dissing: 0.6097\n",
      "\n",
      "Testing similarity for: great\n",
      "\n",
      "Most similar words to 'great':\n",
      "1. grandniece: 0.7862\n",
      "2. eatlon: 0.7047\n",
      "3. tribulation: 0.6270\n",
      "4. lengths: 0.6145\n",
      "5. apocalyptically: 0.5894\n",
      "\n",
      "Testing similarity for: fight\n",
      "\n",
      "Most similar words to 'fight':\n",
      "1. fighting: 0.6571\n",
      "2. fights: 0.6538\n",
      "3. battle: 0.6078\n",
      "4. ensues: 0.5867\n",
      "5. eventually: 0.5668\n",
      "\n",
      "Interactive mode (press Enter to exit):\n",
      "\n",
      "Testing similarity for: king\n",
      "\n",
      "Most similar words to 'king':\n",
      "1. mudbeard: 0.7786\n",
      "2. silvergon: 0.7775\n",
      "3. ahasuerus: 0.7728\n",
      "4. kalakeya: 0.7654\n",
      "5. yemma: 0.7352\n",
      "\n",
      "Testing similarity for: king\n",
      "\n",
      "Most similar words to 'king':\n",
      "1. mudbeard: 0.7786\n",
      "2. silvergon: 0.7775\n",
      "3. ahasuerus: 0.7728\n",
      "4. kalakeya: 0.7654\n",
      "5. yemma: 0.7352\n",
      "\n",
      "Testing similarity for: kiss\n",
      "\n",
      "Most similar words to 'kiss':\n",
      "1. embrace: 0.7031\n",
      "2. passionate: 0.6824\n",
      "3. share: 0.6762\n",
      "4. kisses: 0.6422\n",
      "5. anthia: 0.6276\n",
      "\n",
      "Testing similarity for: kiss\n",
      "\n",
      "Most similar words to 'kiss':\n",
      "1. embrace: 0.7031\n",
      "2. passionate: 0.6824\n",
      "3. share: 0.6762\n",
      "4. kisses: 0.6422\n",
      "5. anthia: 0.6276\n",
      "\n",
      "Testing similarity for: hug\n",
      "\n",
      "Most similar words to 'hug':\n",
      "1. embrace: 0.5575\n",
      "2. kiss: 0.5440\n",
      "3. share: 0.5357\n",
      "4. passionate: 0.4707\n",
      "5. heartfelt: 0.4683\n",
      "\n",
      "Testing similarity for: love\n",
      "\n",
      "Most similar words to 'love':\n",
      "1. falls: 0.6686\n",
      "2. fall: 0.6582\n",
      "3. married: 0.6247\n",
      "4. marry: 0.6217\n",
      "5. loves: 0.6194\n",
      "\n",
      "Testing similarity for: hero\n",
      "\n",
      "Most similar words to 'hero':\n",
      "1. hailed: 0.7665\n",
      "2. taisen: 0.7239\n",
      "3. lauded: 0.7145\n",
      "4. gentlewoman: 0.6988\n",
      "5. feted: 0.6791\n",
      "\n",
      "Testing similarity for: hero\n",
      "\n",
      "Most similar words to 'hero':\n",
      "1. hailed: 0.7665\n",
      "2. taisen: 0.7239\n",
      "3. lauded: 0.7145\n",
      "4. gentlewoman: 0.6988\n",
      "5. feted: 0.6791\n",
      "\n",
      "Testing similarity for: bro\n",
      "\n",
      "Most similar words to 'bro':\n",
      "1. yanzhi: 0.6529\n",
      "2. butterflies: 0.3996\n",
      "3. hex: 0.3961\n",
      "4. mado: 0.3757\n",
      "5. simonetta: 0.3605\n",
      "\n",
      "Testing similarity for: film\n",
      "\n",
      "Most similar words to 'film':\n",
      "1. movie: 0.7953\n",
      "2. donsky: 0.7455\n",
      "3. usire: 0.6735\n",
      "4. story: 0.6702\n",
      "5. ends: 0.6488\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def load_glove_model(model_dir):\n",
    "    embeddings = np.load(f\"{model_dir}/embeddings.npy\")\n",
    "    with open(f\"{model_dir}/word_to_id.pkl\", \"rb\") as f:\n",
    "        word_to_id = pickle.load(f)\n",
    "    with open(f\"{model_dir}/id_to_word.pkl\", \"rb\") as f:\n",
    "        id_to_word = pickle.load(f)\n",
    "    with open(f\"{model_dir}/metadata.pkl\", \"rb\") as f:\n",
    "        metadata = pickle.load(f)\n",
    "    return embeddings, word_to_id, id_to_word, metadata\n",
    "\n",
    "print(\"Select GloVe model:\")\n",
    "print(\"1. English (30k dataset)\")\n",
    "print(\"2. Hindi   (30k dataset)\")\n",
    "\n",
    "choice = input(\"Enter choice (1 or 2, default=1): \").strip() or \"1\"\n",
    "if choice == \"2\":\n",
    "    model_dir = \"embeddings/glove_model_hindi_30k\"\n",
    "    test_words = [\"प्यार\", \"स्कूल\", \"फिल्म\", \"अच्छा\", \"खराब\", \"नायक\"]\n",
    "else:\n",
    "    model_dir = \"wordembeddings/glove_model_30k\"\n",
    "    test_words = [\"action\", \"love\", \"comedy\", \"school\", \"romance\",\n",
    "                  \"movie\", \"film\", \"good\", \"bad\", \"great\", \"fight\"]\n",
    "\n",
    "try:\n",
    "    embeddings, word_to_id, id_to_word, metadata = load_glove_model(model_dir)\n",
    "    print(f\"\\nModel loaded: {model_dir}\")\n",
    "    print(f\"Vocabulary size: {len(word_to_id)}\")\n",
    "    print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Model directory '{model_dir}' not found.\")\n",
    "    exit(1)\n",
    "\n",
    "def most_similar_cpu(word, embeddings, word_to_id, id_to_word, top_n=10):\n",
    "    if word not in word_to_id:\n",
    "        print(f\"'{word}' not in vocabulary\")\n",
    "        return\n",
    "    target_vec = embeddings[word_to_id[word]]\n",
    "    similarities = np.dot(embeddings, target_vec)\n",
    "    norms = np.sqrt(np.sum(embeddings * embeddings, axis=1))\n",
    "    target_norm = np.sqrt(np.dot(target_vec, target_vec))\n",
    "    norms = np.where(norms == 0, 1e-8, norms)\n",
    "    target_norm = target_norm if target_norm != 0 else 1e-8\n",
    "    similarities = similarities / (norms * target_norm)\n",
    "    word_idx = word_to_id[word]\n",
    "    similarities[word_idx] = -1\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_n]\n",
    "    print(f\"\\nMost similar words to '{word}':\")\n",
    "    for i, idx in enumerate(top_indices, 1):\n",
    "        print(f\"{i}. {id_to_word[idx]}: {similarities[idx]:.4f}\")\n",
    "\n",
    "def test_word_similarity(word, embeddings, word_to_id, id_to_word, top_n=5):\n",
    "    print(f\"\\nTesting similarity for: {word}\")\n",
    "    most_similar_cpu(word, embeddings, word_to_id, id_to_word, top_n)\n",
    "\n",
    "def show_vocabulary_sample():\n",
    "    print(\"\\nVocabulary sample (first 20 words):\")\n",
    "    sample_words = list(word_to_id.keys())[:20]\n",
    "    for word in sample_words:\n",
    "        print(word, end=\" \")\n",
    "    print(f\"\\n... and {len(word_to_id) - 20} more words\")\n",
    "\n",
    "show_vocabulary_sample()\n",
    "\n",
    "print(\"\\nTesting predefined words:\")\n",
    "for word in test_words:\n",
    "    test_word_similarity(word, embeddings, word_to_id, id_to_word, top_n=5)\n",
    "\n",
    "print(\"\\nInteractive mode (press Enter to exit):\")\n",
    "while True:\n",
    "    try:\n",
    "        user_word = input(\"Enter a word: \").strip()\n",
    "        if not user_word:\n",
    "            break\n",
    "        test_word_similarity(user_word, embeddings, word_to_id, id_to_word, top_n=5)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nExiting...\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb87971",
   "metadata": {},
   "source": [
    "# Classifier testing with pre-trained model for classification \n",
    "Custom genre prediction with trained models enter 1 for english and 2 for hindi and then enter text using 30k datasets for both hindi and english by default\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ba7f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select language:\n",
      "1. English\n",
      "2. Hindi\n",
      "\n",
      "INTERACTIVE GENRE PREDICTION MODE\n",
      "\n",
      "Input text length: 7 words\n",
      "Predicted genre: Drama (Confidence: 12.88%)\n",
      "\n",
      "Top 5 predictions:\n",
      "  1. Drama                ███░░░░░░░░░░░░░░░░░░░░░░░░░░░  12.88%\n",
      "  2. Mystery              ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░   7.94%\n",
      "  3. Thriller             ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░   7.75%\n",
      "  4. Western              ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░   7.61%\n",
      "  5. Crime                ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░   7.15%\n",
      "\n",
      "\n",
      "Input text length: 36 words\n",
      "Predicted genre: Crime (Confidence: 67.86%)\n",
      "\n",
      "Top 5 predictions:\n",
      "  1. Crime                ████████████████████░░░░░░░░░░  67.86%\n",
      "  2. Western              ████░░░░░░░░░░░░░░░░░░░░░░░░░░  14.07%\n",
      "  3. Action               ████░░░░░░░░░░░░░░░░░░░░░░░░░░  13.63%\n",
      "  4. Thriller             ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   1.66%\n",
      "  5. Horror               ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   0.83%\n",
      "\n",
      "\n",
      "Input text length: 29 words\n",
      "Predicted genre: Comedy (Confidence: 88.28%)\n",
      "\n",
      "Top 5 predictions:\n",
      "  1. Comedy               ██████████████████████████░░░░  88.28%\n",
      "  2. Drama                █░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   5.44%\n",
      "  3. Horror               ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   1.69%\n",
      "  4. Crime                ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   0.78%\n",
      "  5. Short Film           ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   0.58%\n",
      "\n",
      "\n",
      "Input text length: 26 words\n",
      "Predicted genre: Comedy (Confidence: 98.92%)\n",
      "\n",
      "Top 5 predictions:\n",
      "  1. Comedy               █████████████████████████████░  98.92%\n",
      "  2. Horror               ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   0.47%\n",
      "  3. Short Film           ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   0.32%\n",
      "  4. Family               ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   0.08%\n",
      "  5. Drama                ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   0.04%\n",
      "\n",
      "\n",
      "Input text length: 28 words\n",
      "Predicted genre: Drama (Confidence: 37.44%)\n",
      "\n",
      "Top 5 predictions:\n",
      "  1. Drama                ███████████░░░░░░░░░░░░░░░░░░░  37.44%\n",
      "  2. Comedy               █████░░░░░░░░░░░░░░░░░░░░░░░░░  18.46%\n",
      "  3. Romance              ███░░░░░░░░░░░░░░░░░░░░░░░░░░░  12.32%\n",
      "  4. Musical              █░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   5.88%\n",
      "  5. Action               ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   2.92%\n",
      "\n",
      "\n",
      "Input text length: 29 words\n",
      "Predicted genre: Musical (Confidence: 28.42%)\n",
      "\n",
      "Top 5 predictions:\n",
      "  1. Musical              ████████░░░░░░░░░░░░░░░░░░░░░░  28.42%\n",
      "  2. Comedy               ███████░░░░░░░░░░░░░░░░░░░░░░░  25.65%\n",
      "  3. Drama                ████░░░░░░░░░░░░░░░░░░░░░░░░░░  15.03%\n",
      "  4. Romance              ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░   7.59%\n",
      "  5. Fantasy              █░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   5.65%\n",
      "\n",
      "\n",
      "Input text length: 29 words\n",
      "Predicted genre: Musical (Confidence: 28.42%)\n",
      "\n",
      "Top 5 predictions:\n",
      "  1. Musical              ████████░░░░░░░░░░░░░░░░░░░░░░  28.42%\n",
      "  2. Comedy               ███████░░░░░░░░░░░░░░░░░░░░░░░  25.65%\n",
      "  3. Drama                ████░░░░░░░░░░░░░░░░░░░░░░░░░░  15.03%\n",
      "  4. Romance              ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░   7.59%\n",
      "  5. Fantasy              █░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   5.65%\n",
      "\n",
      "\n",
      "Input text length: 30 words\n",
      "Predicted genre: Crime (Confidence: 41.30%)\n",
      "\n",
      "Top 5 predictions:\n",
      "  1. Crime                ████████████░░░░░░░░░░░░░░░░░░  41.30%\n",
      "  2. Science Fiction      █████░░░░░░░░░░░░░░░░░░░░░░░░░  18.19%\n",
      "  3. Mystery              ████░░░░░░░░░░░░░░░░░░░░░░░░░░  14.65%\n",
      "  4. Action               █░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   5.52%\n",
      "  5. Drama                █░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   5.15%\n",
      "\n",
      "\n",
      "Input text length: 31 words\n",
      "Predicted genre: Fantasy (Confidence: 35.00%)\n",
      "\n",
      "Top 5 predictions:\n",
      "  1. Fantasy              ██████████░░░░░░░░░░░░░░░░░░░░  35.00%\n",
      "  2. Drama                ████░░░░░░░░░░░░░░░░░░░░░░░░░░  15.88%\n",
      "  3. Adventure            ███░░░░░░░░░░░░░░░░░░░░░░░░░░░  12.97%\n",
      "  4. Science Fiction      ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░   9.23%\n",
      "  5. Horror               ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░   7.81%\n",
      "\n",
      "\n",
      "Input text length: 32 words\n",
      "Predicted genre: Horror (Confidence: 67.35%)\n",
      "\n",
      "Top 5 predictions:\n",
      "  1. Horror               ████████████████████░░░░░░░░░░  67.35%\n",
      "  2. Thriller             ███░░░░░░░░░░░░░░░░░░░░░░░░░░░  13.07%\n",
      "  3. Drama                █░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   4.65%\n",
      "  4. Mystery              ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   3.01%\n",
      "  5. Comedy               ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   2.31%\n",
      "\n",
      "\n",
      "Input text length: 25 words\n",
      "Predicted genre: Thriller (Confidence: 47.27%)\n",
      "\n",
      "Top 5 predictions:\n",
      "  1. Thriller             ██████████████░░░░░░░░░░░░░░░░  47.27%\n",
      "  2. Action               ███████░░░░░░░░░░░░░░░░░░░░░░░  25.19%\n",
      "  3. Romance              ██████░░░░░░░░░░░░░░░░░░░░░░░░  21.10%\n",
      "  4. Comedy               ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   2.26%\n",
      "  5. Drama                ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   1.02%\n",
      "\n",
      "\n",
      "Input text length: 23 words\n",
      "Predicted genre: Comedy (Confidence: 34.39%)\n",
      "\n",
      "Top 5 predictions:\n",
      "  1. Comedy               ██████████░░░░░░░░░░░░░░░░░░░░  34.39%\n",
      "  2. Crime                ████████░░░░░░░░░░░░░░░░░░░░░░  27.63%\n",
      "  3. Drama                ████░░░░░░░░░░░░░░░░░░░░░░░░░░  15.35%\n",
      "  4. Mystery              ███░░░░░░░░░░░░░░░░░░░░░░░░░░░  13.33%\n",
      "  5. Thriller             ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   2.16%\n",
      "\n",
      "\n",
      "Input text length: 26 words\n",
      "Predicted genre: Crime (Confidence: 31.35%)\n",
      "\n",
      "Top 5 predictions:\n",
      "  1. Crime                █████████░░░░░░░░░░░░░░░░░░░░░  31.35%\n",
      "  2. Comedy               █████░░░░░░░░░░░░░░░░░░░░░░░░░  18.97%\n",
      "  3. Action               ███░░░░░░░░░░░░░░░░░░░░░░░░░░░  12.43%\n",
      "  4. Horror               █░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   5.75%\n",
      "  5. Thriller             █░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   4.94%\n",
      "\n",
      "\n",
      "Input text length: 28 words\n",
      "Predicted genre: Science Fiction (Confidence: 31.67%)\n",
      "\n",
      "Top 5 predictions:\n",
      "  1. Science Fiction      █████████░░░░░░░░░░░░░░░░░░░░░  31.67%\n",
      "  2. Animation            ███████░░░░░░░░░░░░░░░░░░░░░░░  26.07%\n",
      "  3. Fantasy              █████░░░░░░░░░░░░░░░░░░░░░░░░░  18.72%\n",
      "  4. Family               ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░   7.64%\n",
      "  5. Horror               █░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   5.97%\n",
      "\n",
      "\n",
      "Input text length: 21 words\n",
      "Predicted genre: Musical (Confidence: 67.48%)\n",
      "\n",
      "Top 5 predictions:\n",
      "  1. Musical              ████████████████████░░░░░░░░░░  67.48%\n",
      "  2. Romance              ███░░░░░░░░░░░░░░░░░░░░░░░░░░░  12.41%\n",
      "  3. Comedy               █░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   4.44%\n",
      "  4. Drama                █░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   3.68%\n",
      "  5. Animation            ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   3.02%\n",
      "\n",
      "\n",
      "Input text length: 29 words\n",
      "Predicted genre: Drama (Confidence: 27.29%)\n",
      "\n",
      "Top 5 predictions:\n",
      "  1. Drama                ████████░░░░░░░░░░░░░░░░░░░░░░  27.29%\n",
      "  2. Adventure            ███░░░░░░░░░░░░░░░░░░░░░░░░░░░  12.07%\n",
      "  3. War                  ███░░░░░░░░░░░░░░░░░░░░░░░░░░░  11.66%\n",
      "  4. Western              ███░░░░░░░░░░░░░░░░░░░░░░░░░░░  10.40%\n",
      "  5. Action               █░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   5.91%\n",
      "\n",
      "\n",
      "Input text length: 21 words\n",
      "Predicted genre: Drama (Confidence: 56.60%)\n",
      "\n",
      "Top 5 predictions:\n",
      "  1. Drama                ████████████████░░░░░░░░░░░░░░  56.60%\n",
      "  2. Biography            ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░   9.58%\n",
      "  3. Crime                █░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   6.06%\n",
      "  4. Historical           █░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   5.73%\n",
      "  5. Documentary          ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   3.11%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "torch.backends.mps.is_available() and torch.backends.mps.is_built()\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes, pretrained_embeddings=None, filter_sizes=[3,4,5], num_filters=128, dropout=0.5, max_seq_len=600):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(embedding_dim, num_filters, kernel_size=fs) for fs in filter_sizes])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(filter_sizes)*num_filters, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).transpose(1,2)\n",
    "        conv_outputs = []\n",
    "        for conv in self.convs:\n",
    "            pooled = F.max_pool1d(F.relu(conv(x)), kernel_size=conv(x).size(2))\n",
    "            conv_outputs.append(pooled.squeeze(2))\n",
    "        x = torch.cat(conv_outputs, dim=1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "class CNNGenreClassifier:\n",
    "    def __init__(self, model_path, glove_model_dir):\n",
    "        self.device = device\n",
    "        self.embeddings, self.word_to_id, self.id_to_word, self.metadata = self.load_glove_model(glove_model_dir)\n",
    "        self.model_data = self.load_cnn_model(model_path)\n",
    "        self.model = self.build_model()\n",
    "        self.model.eval()\n",
    "    def load_glove_model(self, model_dir):\n",
    "        embeddings = np.load(f\"{model_dir}/embeddings.npy\")\n",
    "        with open(f\"{model_dir}/word_to_id.pkl\", \"rb\") as f: word_to_id = pickle.load(f)\n",
    "        with open(f\"{model_dir}/id_to_word.pkl\", \"rb\") as f: id_to_word = pickle.load(f)\n",
    "        with open(f\"{model_dir}/metadata.pkl\", \"rb\") as f: metadata = pickle.load(f)\n",
    "        return embeddings, word_to_id, id_to_word, metadata\n",
    "    def load_cnn_model(self, model_path):\n",
    "        with open(model_path, \"rb\") as f: return pickle.load(f)\n",
    "    def build_model(self):\n",
    "        model = TextCNN(\n",
    "            vocab_size=self.model_data['vocab_size'],\n",
    "            embedding_dim=self.model_data['embedding_dim'],\n",
    "            num_classes=self.model_data['num_classes'],\n",
    "            pretrained_embeddings=self.embeddings,\n",
    "            filter_sizes=self.model_data['filter_sizes'],\n",
    "            num_filters=self.model_data['num_filters'],\n",
    "            dropout=self.model_data['dropout'],\n",
    "            max_seq_len=self.model_data['max_seq_len']\n",
    "        ).to(self.device)\n",
    "        model.load_state_dict(self.model_data['model_state'])\n",
    "        return model\n",
    "    def text_to_indices(self, text):\n",
    "        tokens = text.lower().split()\n",
    "        indices = [self.word_to_id.get(w,0) for w in tokens]\n",
    "        return indices[:self.model_data['max_seq_len']]\n",
    "    def predict_genre(self, text, top_k=3):\n",
    "        indices = self.text_to_indices(text)\n",
    "        input_tensor = torch.tensor([indices], dtype=torch.long).to(self.device)\n",
    "        max_len = self.model_data['max_seq_len']\n",
    "        if input_tensor.size(1) < max_len:\n",
    "            padding = torch.zeros(1, max_len - input_tensor.size(1), dtype=torch.long).to(self.device)\n",
    "            input_tensor = torch.cat([input_tensor, padding], dim=1)\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_tensor)\n",
    "            probabilities = F.softmax(output, dim=1)\n",
    "        probs_np = probabilities.cpu().numpy()[0]\n",
    "        top_indices = np.argsort(probs_np)[::-1][:top_k]\n",
    "        results = {'predicted_genre': self.model_data['idx_to_label'][top_indices[0]], 'confidence': float(probs_np[top_indices[0]]), 'top_predictions': []}\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            genre = self.model_data['idx_to_label'][idx]\n",
    "            prob = float(probs_np[idx])\n",
    "            results['top_predictions'].append({'rank': i+1, 'genre': genre, 'probability': prob, 'confidence_percent': prob*100})\n",
    "        print(f\"\\nInput text length: {len(text.split())} words\")\n",
    "        print(f\"Predicted genre: {results['predicted_genre']} (Confidence: {results['confidence']*100:.2f}%)\")\n",
    "        print(f\"\\nTop {top_k} predictions:\")\n",
    "        for pred in results['top_predictions']:\n",
    "            bar_length = int(pred['probability']*30)\n",
    "            bar = '█'*bar_length + '░'*(30-bar_length)\n",
    "            print(f\"  {pred['rank']}. {pred['genre']:<20} {bar} {pred['confidence_percent']:6.2f}%\")\n",
    "        return results\n",
    "    def interactive_mode(self):\n",
    "        print(\"\\nINTERACTIVE GENRE PREDICTION MODE\")\n",
    "        while True:\n",
    "            try:\n",
    "                text = input(\"Enter movie plot: \").strip()\n",
    "                if text.lower() in ['quit','exit','q']: break\n",
    "                if not text: continue\n",
    "                if len(text.split()) < 5: continue\n",
    "                self.predict_genre(text, top_k=5)\n",
    "                print()\n",
    "            except KeyboardInterrupt: break\n",
    "            except Exception as e: print(f\"Error: {e}\")\n",
    "\n",
    "def select_language_and_run():\n",
    "    print(\"Select language:\\n1. English\\n2. Hindi\")\n",
    "    choice = input(\"Enter 1 or 2: \").strip()\n",
    "    if choice=='1':\n",
    "        MODEL_PATH = \"models/500itrenglish30k_cnn.pkl\"\n",
    "        GLOVE_MODEL_DIR = \"wordembeddings/glove_model_30k\"\n",
    "    elif choice=='2':\n",
    "        MODEL_PATH = \"models/hindi30k_cnn.pkl\"\n",
    "        GLOVE_MODEL_DIR = \"embeddings/glove_model_hindi_30k\"\n",
    "    else:\n",
    "        print(\"Invalid choice\"); return\n",
    "    if not os.path.exists(MODEL_PATH) or not os.path.exists(GLOVE_MODEL_DIR):\n",
    "        print(\"Model or embedding path not found\"); return\n",
    "    classifier = CNNGenreClassifier(MODEL_PATH, GLOVE_MODEL_DIR)\n",
    "    classifier.interactive_mode()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    select_language_and_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8675aef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ENGLISH ANALOGY TESTS ---\n",
      "king - man + woman = ?\n",
      "  silvergon (0.6332)\n",
      "  mudbeard (0.6270)\n",
      "  ahasuerus (0.6134)\n",
      "  gurumes (0.6111)\n",
      "  queen (0.5938)\n",
      "paris - france + italy = ?\n",
      "  siena (0.4658)\n",
      "  travel (0.4456)\n",
      "  travels (0.4335)\n",
      "  venice (0.4293)\n",
      "  europe (0.3896)\n",
      "walking - walked + swam = ?\n",
      "  street (0.4335)\n",
      "  across (0.4271)\n",
      "  tramline (0.4113)\n",
      "  walk (0.3881)\n",
      "  spots (0.3701)\n",
      "\n",
      "--- HINDI ANALOGY TESTS ---\n",
      "राजा - पुरुष + महिला = ?\n",
      "  किस (0.4186)\n",
      "  महाराजा (0.3901)\n",
      "  साझा (0.3728)\n",
      "  थूका (0.3726)\n",
      "  होमपेज (0.3674)\n",
      "दिल्ली - भारत + पाकिस्तान = ?\n",
      "  भजनपुरा (0.5469)\n",
      "  एनसीआर (0.5383)\n",
      "  भोगल (0.4794)\n",
      "  डीयू (0.4322)\n",
      "  रंगला (0.4301)\n",
      "लड़का - आदमी + महिला = ?\n",
      "  मुखियाओं (0.5530)\n",
      "  प्रशिक्षु (0.4798)\n",
      "  आया (0.4351)\n",
      "  कॉर्निया (0.4234)\n",
      "  बॉयकॉट (0.4205)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class WordAnalogyTester:\n",
    "    def __init__(self, glove_model_dir):\n",
    "        self.embeddings, self.word_to_id, self.id_to_word, self.metadata = self.load_glove_model(glove_model_dir)\n",
    "        self.embeddings = self.embeddings / np.linalg.norm(self.embeddings, axis=1, keepdims=True)\n",
    "    def load_glove_model(self, model_dir):\n",
    "        embeddings = np.load(f\"{model_dir}/embeddings.npy\")\n",
    "        with open(f\"{model_dir}/word_to_id.pkl\", \"rb\") as f: word_to_id = pickle.load(f)\n",
    "        with open(f\"{model_dir}/id_to_word.pkl\", \"rb\") as f: id_to_word = pickle.load(f)\n",
    "        with open(f\"{model_dir}/metadata.pkl\", \"rb\") as f: metadata = pickle.load(f)\n",
    "        return embeddings, word_to_id, id_to_word, metadata\n",
    "    def analogy(self, a, b, c, top_k=5):\n",
    "        if any(w not in self.word_to_id for w in [a,b,c]): return None\n",
    "        vec = self.embeddings[self.word_to_id[a]] - self.embeddings[self.word_to_id[b]] + self.embeddings[self.word_to_id[c]]\n",
    "        vec = vec / np.linalg.norm(vec)\n",
    "        sims = np.dot(self.embeddings, vec)\n",
    "        best = sims.argsort()[::-1]\n",
    "        results = []\n",
    "        for idx in best:\n",
    "            word = self.id_to_word[idx]\n",
    "            if word in [a,b,c]: continue\n",
    "            results.append((word, float(sims[idx])))\n",
    "            if len(results) >= top_k: break\n",
    "        return results\n",
    "\n",
    "def run_tests(lang):\n",
    "    if lang==\"english\":\n",
    "        GLOVE_MODEL_DIR = \"wordembeddings/glove_model_30k\"\n",
    "        tester = WordAnalogyTester(GLOVE_MODEL_DIR)\n",
    "        examples = [(\"king\",\"man\",\"woman\"),(\"paris\",\"france\",\"italy\"),(\"walking\",\"walked\",\"swam\")]\n",
    "    elif lang==\"hindi\":\n",
    "        GLOVE_MODEL_DIR = \"embeddings/glove_model_hindi_30k\"\n",
    "        tester = WordAnalogyTester(GLOVE_MODEL_DIR)\n",
    "        examples = [(\"राजा\",\"पुरुष\",\"महिला\"),(\"दिल्ली\",\"भारत\",\"पाकिस्तान\"),(\"लड़का\",\"आदमी\",\"महिला\")]\n",
    "    else:\n",
    "        return\n",
    "    print(f\"\\n--- {lang.upper()} ANALOGY TESTS ---\")\n",
    "    for a,b,c in examples:\n",
    "        result = tester.analogy(a,b,c,top_k=5)\n",
    "        if result:\n",
    "            print(f\"{a} - {b} + {c} = ?\")\n",
    "            for r in result:\n",
    "                print(f\"  {r[0]} ({r[1]:.4f})\")\n",
    "        else:\n",
    "            print(f\"Words not found for {a},{b},{c}\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    run_tests(\"english\")\n",
    "    run_tests(\"hindi\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
